{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import sklearn\n",
    "import jsonlines\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(pyobj, keystring=''): \n",
    "    if type(pyobj) == dict: \n",
    "        keystring = keystring + '_' if keystring else keystring \n",
    "        for k in pyobj: \n",
    "            yield from flatten_dict(pyobj[k], keystring + str(k)) \n",
    "    else: \n",
    "        yield keystring, pyobj \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(hash_list):\n",
    "    hashtags = ''\n",
    "    for item in hash_list:\n",
    "        hashtags = hashtags + ' ' + item['text']\n",
    "        hashtags = hashtags.strip()\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_names = ['lang', 'quoted_status_lang', 'created_at', 'id_str', 'full_text', \n",
    "             'entities_hashtags', 'entities_urls', 'entities_media',\n",
    "             'quoted_status_full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = '/Volumes/LACIE_SHARE/python_scripts/Data/coronavirus-tweet-id-2020-02-01-00.jsonl'\n",
    "my_file2 = '/Volumes/LACIE_SHARE/python_scripts/Data/coronavirus-tweet-id-2020-02-01-01.jsonl'\n",
    "my_file3 = '/Volumes/LACIE_SHARE/python_scripts/Data/coronavirus-tweet-id-2020-02-01-02.jsonl'\n",
    "my_file4 = '/Volumes/LACIE_SHARE/python_scripts/Data/coronavirus-tweet-id-2020-02-01-03.jsonl'\n",
    "my_file5 = '/Volumes/LACIE_SHARE/python_scripts/Data/coronavirus-tweet-id-2020-02-01-09.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47409 records\n",
      "Loaded 94818 records\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "\n",
    "\n",
    "for file in [my_file, my_file2]:\n",
    "    with jsonlines.open(my_file) as f:\n",
    "        for line in f:\n",
    "            # keep only if english and not a retweet\n",
    "            if line['lang'] == 'en':\n",
    "                data.append(dict(flatten_dict(line)))\n",
    "    print('Loaded {} records'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only = []\n",
    "for d in data:\n",
    "    text_only.append(d.get('full_text',''))\n",
    "\n",
    "#We should eventually remove URLs and the username of someone if we are retweeting.\n",
    "text_only_df = pd.DataFrame(text_only, columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_df = text_only_df.sample(n = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                     text  \\\n",
       "8429   RT @DanyalGilani: Salute to these Chinese doct...   \n",
       "46184  RT @SethAbramson: If you've read *anything* ab...   \n",
       "18071  RT @Huh_My_Rahhhhh: Father can’t touch his lit...   \n",
       "90076  RT @pautergeist: population to this highly con...   \n",
       "49317  RT @AJListeningPost: The coronavirus outbreak ...   \n",
       "...                                                  ...   \n",
       "67607  RT @Marfoogle: IF THIS IS REAL? then the futur...   \n",
       "7229   RT @popplioikawa: the flu: *results in 500,000...   \n",
       "81723  RT @catcontentonly: Your racist jokes about dy...   \n",
       "22329  RT @mercola: Read my thoughts on the Coronavir...   \n",
       "73459  RT @Pdiddy1805: @SteveKerr You sure do have a ...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "8429   [salute, chinese, doctor, working, tirelessly,...   \n",
       "46184  [read, anything, trump, activity, china, turke...   \n",
       "18071  [father, touch, little, baby, infected, corona...   \n",
       "90076  [population, highly, contagious, virus, put, m...   \n",
       "49317  [coronavirus, outbreak, china, public, health,...   \n",
       "...                                                  ...   \n",
       "67607                                     [real, future]   \n",
       "7229   [flu, result, 500, 000, hospitalization, 30, 0...   \n",
       "81723  [racist, joke, dying, coronavirus, tired, flu,...   \n",
       "22329                       [read, thought, coronavirus]   \n",
       "73459  [sure, lot, thought, except, come, china, bent...   \n",
       "\n",
       "                                                   clean  \n",
       "8429   salute chinese doctor working tirelessly wake ...  \n",
       "46184  read anything trump activity china turkey iraq...  \n",
       "18071  father touch little baby infected corona virus...  \n",
       "90076  population highly contagious virus put many so...  \n",
       "49317  coronavirus outbreak china public health disas...  \n",
       "...                                                  ...  \n",
       "67607                                        real future  \n",
       "7229   flu result 500 000 hospitalization 30 000 deat...  \n",
       "81723  racist joke dying coronavirus tired flu killed...  \n",
       "22329                           read thought coronavirus  \n",
       "73459       sure lot thought except come china bent knee  \n",
       "\n",
       "[5000 rows x 3 columns]>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_only_df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5000 entries, 8429 to 73459\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   tokens  5000 non-null   object\n",
      " 2   clean   5000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 156.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Check for and remove missing values and blank strings\n",
    "print(text_only_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "sw.add('rt')\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    x = re.sub(r'http\\S+', '', x) ## remove any urls in the tweets\n",
    "    x = re.sub(r'@\\S+','', x) #remove any usernames in the tweets\n",
    "    x = emoji_pattern.sub(r'', x)\n",
    "    \n",
    "    x = x.lower()\n",
    "    x = x.replace('\\r', '') ## REmove \\r \\n\n",
    "    x = x.replace('\\n', '') #remove \\n\n",
    "    x = x.replace(\"\\'\", \"'\") #replace \\' with '\n",
    "    x = x.replace(\"#\", \"\") ## Remove hashtags\n",
    "    #x = x.replace(\"@\", \"\") ## remove the @ symbol for retweets\n",
    "    tokens = wordpunct_tokenize(x)\n",
    "    tokens = [tok for tok in tokens if tok.isalnum()]\n",
    "    \n",
    "    tokens = [tok for tok in tokens if tok not in sw]\n",
    "    \n",
    "    tokens = [wn.lemmatize(tok) for tok in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "\n",
    "text_only_df['tokens'] = text_only_df['text'].apply(tokenize)\n",
    "text_only_df['clean'] = text_only_df['tokens'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8429     RT @DanyalGilani: Salute to these Chinese doct...\n",
       "46184    RT @SethAbramson: If you've read *anything* ab...\n",
       "18071    RT @Huh_My_Rahhhhh: Father can’t touch his lit...\n",
       "90076    RT @pautergeist: population to this highly con...\n",
       "49317    RT @AJListeningPost: The coronavirus outbreak ...\n",
       "                               ...                        \n",
       "67607    RT @Marfoogle: IF THIS IS REAL? then the futur...\n",
       "7229     RT @popplioikawa: the flu: *results in 500,000...\n",
       "81723    RT @catcontentonly: Your racist jokes about dy...\n",
       "22329    RT @mercola: Read my thoughts on the Coronavir...\n",
       "73459    RT @Pdiddy1805: @SteveKerr You sure do have a ...\n",
       "Name: text, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.133*\"coronavirus\" + 0.039*\"case\" + 0.037*\"u\" + 0.026*\"new\" + 0.020*\"health\" + 0.016*\"first\" + 0.013*\"amp\" + 0.013*\"update\" + 0.013*\"confirmed\" + 0.013*\"say\"'), (1, '0.037*\"one\" + 0.025*\"mask\" + 0.024*\"come\" + 0.023*\"mean\" + 0.018*\"well\" + 0.016*\"foreign\" + 0.014*\"information\" + 0.014*\"market\" + 0.013*\"hedge\" + 0.013*\"still\"'), (2, '0.000*\"pc\" + 0.000*\"ya\" + 0.000*\"gimme\" + 0.000*\"todragkoreanartistsstopusingcoronavirus\" + 0.000*\"peed\" + 0.000*\"poop\" + 0.000*\"knowing\" + 0.000*\"stopusingcoronavirus\" + 0.000*\"egg\" + 0.000*\"log\"'), (3, '0.062*\"coronavirus\" + 0.054*\"wuhan\" + 0.035*\"chinese\" + 0.024*\"outbreak\" + 0.021*\"state\" + 0.018*\"man\" + 0.018*\"report\" + 0.015*\"time\" + 0.014*\"united\" + 0.013*\"said\"'), (4, '0.000*\"pc\" + 0.000*\"ya\" + 0.000*\"gimme\" + 0.000*\"todragkoreanartistsstopusingcoronavirus\" + 0.000*\"peed\" + 0.000*\"poop\" + 0.000*\"knowing\" + 0.000*\"stopusingcoronavirus\" + 0.000*\"egg\" + 0.000*\"log\"'), (5, '0.060*\"000\" + 0.046*\"china\" + 0.036*\"trump\" + 0.033*\"death\" + 0.032*\"flu\" + 0.032*\"alone\" + 0.029*\"people\" + 0.026*\"let\" + 0.025*\"read\" + 0.025*\"year\"'), (6, '0.032*\"day\" + 0.025*\"2\" + 0.023*\"hiv\" + 0.020*\"go\" + 0.019*\"twitter\" + 0.018*\"c\" + 0.017*\"3\" + 0.015*\"zerohedge\" + 0.014*\"immediately\" + 0.014*\"especially\"'), (7, '0.120*\"china\" + 0.053*\"virus\" + 0.018*\"corona\" + 0.015*\"face\" + 0.015*\"baby\" + 0.013*\"know\" + 0.013*\"hit\" + 0.011*\"thought\" + 0.011*\"infection\" + 0.011*\"someone\"'), (8, '0.000*\"pc\" + 0.000*\"ya\" + 0.000*\"gimme\" + 0.000*\"todragkoreanartistsstopusingcoronavirus\" + 0.000*\"peed\" + 0.000*\"poop\" + 0.000*\"knowing\" + 0.000*\"stopusingcoronavirus\" + 0.000*\"egg\" + 0.000*\"log\"'), (9, '0.000*\"pc\" + 0.000*\"ya\" + 0.000*\"gimme\" + 0.000*\"todragkoreanartistsstopusingcoronavirus\" + 0.000*\"peed\" + 0.000*\"poop\" + 0.000*\"knowing\" + 0.000*\"stopusingcoronavirus\" + 0.000*\"egg\" + 0.000*\"log\"')]\n"
     ]
    }
   ],
   "source": [
    "## Run LDA topic modeling on the tweet text\n",
    "\n",
    "\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "ntopics = 10\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(text_only_df['tokens'])\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in text_only_df['tokens']]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=ntopics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "\n",
    "ldatopics = lda_model.show_topics(formatted=False)\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data, kwds)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text/html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     formatter.for_type(PreparedData,\n\u001b[0;32m--> 313\u001b[0;31m                        lambda data, kwds=kwargs: prepared_data_to_html(data, **kwds))\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                            \u001b[0mvis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pyLDAvis/utils.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                        x                   y  topics  cluster       Freq\n",
       "topic                                                                    \n",
       "0     -0.239039+0.000000j  0.282500+0.000000j       1        1  26.768891\n",
       "5     -0.303535+0.000000j -0.263612+0.000000j       2        1  18.169238\n",
       "3     -0.104966+0.000000j  0.212816+0.000000j       3        1  16.903101\n",
       "7     -0.077673+0.000000j -0.187912+0.000000j       4        1  15.673923\n",
       "6      0.110952+0.000000j -0.017975+0.000000j       5        1   9.515220\n",
       "1      0.108597+0.000000j -0.028831+0.000000j       6        1   9.044521\n",
       "4      0.126416+0.000000j  0.000753+0.000000j       7        1   0.981277\n",
       "8      0.126416+0.000000j  0.000753+0.000000j       8        1   0.981277\n",
       "9      0.126416+0.000000j  0.000753+0.000000j       9        1   0.981277\n",
       "2      0.126416+0.000000j  0.000753+0.000000j      10        1   0.981277, topic_info=             Term         Freq        Total Category  logprob  loglift\n",
       "2     coronavirus  2558.000000  2558.000000  Default   30.000  30.0000\n",
       "13          china  1507.000000  1507.000000  Default   29.000  29.0000\n",
       "179           000   604.000000   604.000000  Default   28.000  28.0000\n",
       "9           wuhan   509.000000   509.000000  Default   27.000  27.0000\n",
       "31          virus   463.000000   463.000000  Default   26.000  26.0000\n",
       "...           ...          ...          ...      ...      ...      ...\n",
       "1670          fun     0.079938     1.938974  Topic10   -8.826   1.4354\n",
       "2655           jr     0.079937     1.945599  Topic10   -8.826   1.4320\n",
       "6724   aliexpress     0.079935     1.082768  Topic10   -8.826   2.0180\n",
       "6726          wig     0.079935     1.082768  Topic10   -8.826   2.0180\n",
       "6725        bouta     0.079935     1.082768  Topic10   -8.826   2.0180\n",
       "\n",
       "[938 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "179       2  0.998881        000\n",
       "942       1  0.993732          1\n",
       "273       4  0.992120         10\n",
       "354       2  0.994853         11\n",
       "1969      4  0.989538         14\n",
       "...     ...       ...        ...\n",
       "9         3  0.998924      wuhan\n",
       "119       2  0.996310       year\n",
       "1236      6  0.975939        yet\n",
       "145       6  0.979675       zero\n",
       "542       5  0.986997  zerohedge\n",
       "\n",
       "[194 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 6, 4, 8, 7, 2, 5, 9, 10, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: remove retweets?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
