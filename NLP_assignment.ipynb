{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>some talented actresses are blessed with a dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>this has been an extraordinary year for austra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>according to hollywood movies made in last few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>my first press screening of 1998 and already i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   neg  how do films like mouse hunt get into theatres...\n",
       "1   neg  some talented actresses are blessed with a dem...\n",
       "2   pos  this has been an extraordinary year for austra...\n",
       "3   pos  according to hollywood movies made in last few...\n",
       "4   neg  my first press screening of 1998 and already i..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import the data\n",
    "df = pd.read_csv( '/Volumes/LACIE_SHARE/python_scripts/Data/ratings.tsv', sep='\\t', header = 0)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   8000 non-null   object\n",
      " 1   review  7945 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 125.1+ KB\n",
      "None\n",
      "0 blank labels\n",
      "0 blank reviews\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7945 entries, 0 to 7999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   7945 non-null   object\n",
      " 1   review  7945 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 186.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Check for and remove missing values and blank strings\n",
    "print(df.info())\n",
    "\n",
    "## a few null reviews.\n",
    "\n",
    "print('{} blank labels'.format(np.sum(df['label']==\"\")))\n",
    "print('{} blank reviews'.format(np.sum(df['review'] == \"\")))\n",
    "\n",
    "\n",
    "\n",
    "df = df.dropna(axis=0)\n",
    "print(df.info())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    x = x.lower()\n",
    "    x = x.replace('\\r', '') ## REmove \\r \\n\n",
    "    x = x.replace('\\n', '') #remove \\n\n",
    "    x = x.replace(\"\\'\", \"'\") #replace \\' with '\n",
    "    tokens = wordpunct_tokenize(x)\n",
    "    tokens = [tok for tok in tokens if tok.isalnum()]\n",
    "    \n",
    "    tokens = [tok for tok in tokens if tok not in sw]\n",
    "    \n",
    "    tokens = [wn.lemmatize(tok) for tok in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "\n",
    "df['tokens'] = df['review'].apply(tokenize)\n",
    "df['clean'] = df['tokens'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into a training set and a test set. \n",
    "##Use test size=0.33, stratify=y, and random state=801 (where y is the label positive or negative)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.33, stratify=df.label, random_state=801)\n",
    "\n",
    "y_train = (train['label']=='pos').astype(int)\n",
    "y_test = (test['label']=='pos').astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the data using TD-IDF. Be sure that all model development is with the training data \n",
    "## (fit the TD-IDF transformer on the training data, then transform to both training and test data).\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf.fit(train)\n",
    "\n",
    "tf_idf_train = tf_idf.transform(train['clean'])\n",
    "tf_idf_test = tf_idf.transform(test['clean'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a machine learning classifier. Try out various models including:\n",
    "## Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "linear_svc = SVC(kernel='linear')  \n",
    "linear_svc.fit(tf_idf_train, y_train) \n",
    "\n",
    "linear_predict = linear_svc.predict(tf_idf_test)\n",
    "linear_predict_train = linear_svc.predict(tf_idf_train)\n",
    "\n",
    "sigmoid_svc = SVC(kernel='sigmoid')  \n",
    "sigmoid_svc.fit(tf_idf_train, y_train) \n",
    "\n",
    "sigmoid_predict_train = sigmoid_svc.predict(tf_idf_train)\n",
    "sigmoid_predict = sigmoid_svc.predict(tf_idf_test)\n",
    "\n",
    "\n",
    "\n",
    "## Multilayer Perceptron\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "mlp.fit(tf_idf_train, y_train)\n",
    "\n",
    "mlp_predict = mlp.predict(tf_idf_test)\n",
    "mlp_predict_train = mlp.predict(tf_idf_train)\n",
    "\n",
    "\n",
    "\n",
    "## Multinomial Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(tf_idf_train, y_train)\n",
    "\n",
    "nb_predict = nb.predict(tf_idf_test)\n",
    "nb_predict_train = nb.predict(tf_idf_train)\n",
    "\n",
    "## voting classifier\n",
    "vc = VotingClassifier(estimators=[\n",
    "        ('svc', sigmoid_svc), ('mlp', mlp), ('gnb', nb)],\n",
    "        voting='hard', \n",
    "        weights=[1,1,2])\n",
    "vc.fit(tf_idf_train, y_train)\n",
    "vc_predict = vc.predict(tf_idf_test)\n",
    "vc_predict_train = vc.predict(tf_idf_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(actual, predicted, model_name):\n",
    "    print(\"metrics for {}:\".format(model_name))\n",
    "    print(metrics.confusion_matrix(predicted, actual))\n",
    "    print('we had an accuracy of {}, precision of {}, and recall of {}'.format(metrics.accuracy_score(predicted, actual),\n",
    "                                                                           metrics.precision_score(predicted, actual),\n",
    "                                                                         metrics.recall_score(predicted, actual)))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for linear support vector classifier:\n",
      "[[ 235  159]\n",
      " [2427 2502]]\n",
      "we had an accuracy of 0.5141837309787713, precision of 0.9402480270574972, and recall of 0.5076080340839927\n",
      "\n",
      "metrics for sigmoid support vector classifier:\n",
      "[[ 259  183]\n",
      " [2403 2478]]\n",
      "we had an accuracy of 0.5141837309787713, precision of 0.9312288613303269, and recall of 0.5076828518746158\n",
      "\n",
      "metrics for multilayer perceptron:\n",
      "[[ 259  183]\n",
      " [2403 2478]]\n",
      "we had an accuracy of 0.5141837309787713, precision of 0.9312288613303269, and recall of 0.5076828518746158\n",
      "\n",
      "metrics for naive beyes:\n",
      "[[2622 2623]\n",
      " [  40   38]]\n",
      "we had an accuracy of 0.4997182040202893, precision of 0.014280345734686208, and recall of 0.48717948717948717\n",
      "\n",
      "metrics for voting classifier:\n",
      "[[2652 2650]\n",
      " [  10   11]]\n",
      "we had an accuracy of 0.5002817959797107, precision of 0.004133784291619692, and recall of 0.5238095238095238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_train, linear_predict_train, \"linear support vector classifier\") \n",
    "print_metrics(y_train, sigmoid_predict_train, \"sigmoid support vector classifier\") \n",
    "print_metrics(y_train, mlp_predict_train, \"multilayer perceptron\") \n",
    "print_metrics(y_train, nb_predict_train, \"naive beyes\") \n",
    "print_metrics(y_train, vc_predict_train, 'voting classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our classifiers did not do very well on our data for raw accuracy. Even on our training data, they only did a little bit better than a naive guess. Some of the precision rates were fairly high, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for linear support vector classifier:\n",
      "[[ 100   82]\n",
      " [1211 1229]]\n",
      "we had an accuracy of 0.5068649885583524, precision of 0.9374523264683448, and recall of 0.5036885245901639\n",
      "\n",
      "metrics for sigmoid support vector classifier:\n",
      "[[ 114  100]\n",
      " [1197 1211]]\n",
      "we had an accuracy of 0.5053394355453852, precision of 0.92372234935164, and recall of 0.502906976744186\n",
      "\n",
      "metrics for multilayer perceptron:\n",
      "[[ 114  100]\n",
      " [1197 1211]]\n",
      "we had an accuracy of 0.5053394355453852, precision of 0.92372234935164, and recall of 0.502906976744186\n",
      "\n",
      "metrics for naive beyes:\n",
      "[[1291 1287]\n",
      " [  20   24]]\n",
      "we had an accuracy of 0.5015255530129672, precision of 0.018306636155606407, and recall of 0.5454545454545454\n",
      "\n",
      "metrics for voting classifier:\n",
      "[[1307 1308]\n",
      " [   4    3]]\n",
      "we had an accuracy of 0.4996186117467582, precision of 0.002288329519450801, and recall of 0.42857142857142855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, linear_predict, \"linear support vector classifier\") \n",
    "print_metrics(y_test, sigmoid_predict, \"sigmoid support vector classifier\") \n",
    "print_metrics(y_test, mlp_predict, \"multilayer perceptron\") \n",
    "print_metrics(y_test, nb_predict, \"naive beyes\") \n",
    "print_metrics(y_test, vc_predict, 'voting classifier')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What model performs the best? \n",
    "## Experiment with changing the hyper-parameters, changing the vectorizer, \n",
    "## adding bi-grams and/or using a voting classifier to increase model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Volumes/LACIE_SHARE/Software/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "## Using Vader sentiment analysis, predict whether or not the movie review is positive or negative. \n",
    "# (Use a positive compound score for “positive” and a negative compound score for “negative”).\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "train['scores'] = train['review'].apply(lambda x: sid.polarity_scores(x))\n",
    "train['compound'] = train['scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "test['scores'] = test['review'].apply(lambda x: sid.polarity_scores(x))\n",
    "test['compound'] = test['scores'].apply(lambda x: x['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for vader sentiment analysis:\n",
      "[[ 722  177]\n",
      " [ 589 1134]]\n",
      "we had an accuracy of 0.7078565980167811, precision of 0.8649885583524027, and recall of 0.6581543818920488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## How does the accuracy of the sentiment analysis compare with that of the predictive model?\n",
    "vader_test_predict = (test['compound'] > 0).astype(int)\n",
    "\n",
    "print_metrics(y_test, vader_test_predict, 'vader sentiment analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vader predictions were much better than any of our predictive models for raw accuracy, but they had lower precision rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for textblob sentiment analysis:\n",
      "[[ 599   59]\n",
      " [ 712 1252]]\n",
      "we had an accuracy of 0.7059496567505721, precision of 0.9549961861174676, and recall of 0.6374745417515275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Try doing sentiment analysis with the TextBlob library. \n",
    "## How does the accuracy of TextBlob sentiments compare with Vader and the predictive model?\n",
    "from textblob import TextBlob\n",
    "\n",
    "textblob_test = test['review'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "\n",
    "textblob_test_predict = (textblob_test > 0).astype(int)\n",
    "\n",
    "print_metrics(y_test, textblob_test_predict, 'textblob sentiment analysis')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textblob model worked pretty similar to Vader, but with a higher precision rate and lower recall rate. The textblob model had a precision rate and accuracy rate higher than our predictive models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.039*\"br\" + 0.030*\"movie\" + 0.015*\"film\" + 0.012*\"one\" + 0.008*\"like\" + 0.007*\"time\" + 0.006*\"good\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"really\"'), (1, '0.012*\"killer\" + 0.010*\"cop\" + 0.009*\"police\" + 0.006*\"monster\" + 0.006*\"killing\" + 0.006*\"chase\" + 0.005*\"blood\" + 0.005*\"detective\" + 0.005*\"island\" + 0.004*\"rex\"'), (2, '0.016*\"film\" + 0.008*\"life\" + 0.007*\"performance\" + 0.006*\"story\" + 0.005*\"best\" + 0.005*\"role\" + 0.004*\"love\" + 0.004*\"wonderful\" + 0.004*\"great\" + 0.004*\"world\"')]\n"
     ]
    }
   ],
   "source": [
    "## Run LDA topic modeling using gensim on the movie reviews. \n",
    "\n",
    "## How many topics are there? What are the most common words in each topic?\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "ntopics = 3\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=ntopics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "## Does this create bigram topics?\n",
    "\n",
    "ldatopics = lda_model.show_topics(formatted=False)\n",
    "print(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el225031403256893884962092373774\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el225031403256893884962092373774_data = {\"mdsDat\": {\"x\": [0.2710847385306492, -0.019351030312872528, -0.25173370821777674], \"y\": [0.0977823277791896, -0.2199923212429876, 0.122209993463798], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [76.10650994187783, 18.311544452668873, 5.581945605453287]}, \"tinfo\": {\"Term\": [\"br\", \"movie\", \"film\", \"life\", \"performance\", \"story\", \"killer\", \"like\", \"cop\", \"best\", \"role\", \"time\", \"action\", \"police\", \"love\", \"world\", \"woman\", \"hero\", \"wonderful\", \"family\", \"great\", \"good\", \"young\", \"would\", \"get\", \"really\", \"even\", \"see\", \"make\", \"monster\", \"br\", \"movie\", \"like\", \"time\", \"good\", \"really\", \"get\", \"would\", \"even\", \"see\", \"make\", \"people\", \"show\", \"watch\", \"seen\", \"made\", \"bad\", \"think\", \"ever\", \"first\", \"much\", \"scene\", \"thing\", \"could\", \"know\", \"acting\", \"go\", \"say\", \"plot\", \"actor\", \"one\", \"way\", \"film\", \"character\", \"story\", \"well\", \"great\", \"supporting\", \"tale\", \"cinderella\", \"beauty\", \"social\", \"portrayal\", \"society\", \"oliver\", \"musical\", \"singing\", \"culture\", \"beautifully\", \"african\", \"touching\", \"welles\", \"french\", \"powerful\", \"antwone\", \"de\", \"nature\", \"adaptation\", \"portrayed\", \"arthur\", \"germany\", \"delight\", \"political\", \"italian\", \"western\", \"tony\", \"handsome\", \"cagney\", \"capture\", \"german\", \"lovely\", \"stage\", \"city\", \"superb\", \"wonderful\", \"moving\", \"performance\", \"emotional\", \"beautiful\", \"role\", \"young\", \"life\", \"oscar\", \"strong\", \"mr\", \"perfect\", \"art\", \"world\", \"film\", \"family\", \"woman\", \"true\", \"story\", \"best\", \"war\", \"love\", \"dance\", \"drama\", \"great\", \"u\", \"american\", \"man\", \"picture\", \"work\", \"play\", \"excellent\", \"also\", \"character\", \"many\", \"year\", \"killer\", \"cop\", \"killing\", \"police\", \"detective\", \"island\", \"werewolf\", \"monster\", \"blood\", \"serial\", \"murder\", \"fu\", \"sea\", \"bullet\", \"kung\", \"terrorist\", \"military\", \"chase\", \"scientist\", \"rex\", \"disney\", \"fly\", \"mountain\", \"soldier\", \"gang\", \"officer\", \"fighting\", \"attack\", \"hunt\", \"legend\", \"army\", \"cliffhanger\", \"ray\", \"attacked\", \"dahmer\", \"theodore\", \"btk\", \"hero\", \"force\"], \"Freq\": [36781.0, 28028.0, 17462.0, 3473.0, 2152.0, 4937.0, 795.0, 7914.0, 695.0, 3480.0, 1514.0, 6720.0, 1812.0, 609.0, 3530.0, 1731.0, 2067.0, 717.0, 1012.0, 1475.0, 4035.0, 5215.0, 1033.0, 4988.0, 4968.0, 4901.0, 4841.0, 4743.0, 4684.0, 416.0, 36780.22579139332, 28027.402266716144, 7913.597302632247, 6719.065689493532, 5214.264868909123, 4900.47443926179, 4966.941592377843, 4986.9240818482285, 4839.847818875832, 4741.753704008091, 4683.285645543338, 4558.213209541709, 4120.7060340438675, 4124.171930844093, 4029.1307326148626, 3813.5977802410134, 3421.5127593018933, 3426.6138048753282, 3539.4381989605718, 3713.8691700496865, 3437.6305298854995, 3296.668812851762, 3084.6581717622557, 2979.219500948887, 2970.512502539369, 3062.362252810021, 2903.098718736175, 2855.1370411260555, 2739.7047295955435, 2901.2531591843704, 10765.633943113857, 3664.5184763576763, 13834.601918162469, 3786.8076525985407, 3585.7814996711386, 3118.7510345744863, 3137.081603398006, 535.2703272435916, 443.9283867902567, 389.5493222688781, 474.227089874417, 360.49729046788644, 289.5049475384283, 305.864363675797, 297.7415309228205, 319.1063678992592, 286.681594501681, 280.98759059595744, 279.21346332113535, 277.6199587374786, 277.06674442500474, 333.0101487171478, 271.81762838386913, 240.81045356011262, 242.41598568116163, 361.8851215417569, 241.76013114218338, 222.77058867989868, 254.90554821351515, 263.5056567095477, 293.5807104872962, 259.7315779393228, 350.99160536022146, 272.68620525084395, 252.04000708264678, 238.68988604158, 227.19816364843663, 326.4200539700227, 274.8081615699905, 316.5340471244245, 308.59340928972347, 275.4335935143883, 512.5259657931807, 536.5958195125467, 908.8089009405509, 361.7645579261974, 1551.7879480384784, 353.0370990342468, 646.4794535358542, 1033.8301996177536, 751.1047739894321, 1807.3941330598243, 336.74112226077074, 415.8971758480679, 470.60944770164707, 523.1491375231345, 590.2151052829694, 879.4896953459829, 3627.0130656066617, 786.5630023732375, 878.0320311846389, 580.5699112230683, 1350.7252562904423, 1110.00439347182, 523.4768420604497, 964.0671286581123, 478.967407018939, 529.8856590217803, 897.9537208967167, 627.0340962305663, 522.0536620803871, 664.7040288977198, 512.1762500763242, 609.7812620378834, 542.1634949699736, 518.2561571262218, 603.5777779371318, 616.0812284756246, 532.3770625080151, 520.0952959920465, 793.8463319188544, 694.0094456682019, 412.7227506951831, 607.2728585853386, 329.53834954901953, 316.0901677403835, 269.7799526303798, 414.0184921184842, 332.2170906149597, 228.36931369456948, 244.49197110877162, 209.04521260027084, 236.12788258039905, 198.76268933600866, 177.91853472435446, 196.0790597649206, 174.0682937532003, 395.9361032827482, 182.13418346755532, 298.96375324466743, 277.5182965243232, 188.7042704842097, 150.99737712018884, 144.98168812361655, 207.56736637245962, 217.0814517735985, 143.3077585226535, 219.29262170010517, 151.79581698549393, 142.7519866089678, 193.32606454235585, 165.20909231129332, 161.3414060864744, 165.64514377537827, 204.422843423872, 224.90443390372886, 203.9730050972296, 266.27353652128966, 183.58130879536893], \"Total\": [36781.0, 28028.0, 17462.0, 3473.0, 2152.0, 4937.0, 795.0, 7914.0, 695.0, 3480.0, 1514.0, 6720.0, 1812.0, 609.0, 3530.0, 1731.0, 2067.0, 717.0, 1012.0, 1475.0, 4035.0, 5215.0, 1033.0, 4988.0, 4968.0, 4901.0, 4841.0, 4743.0, 4684.0, 416.0, 36781.79023293279, 28028.686222438926, 7914.885005262176, 6720.400215983582, 5215.561677293549, 4901.727110582378, 4968.236500604523, 4988.228106236935, 4841.128557854014, 4743.066614918789, 4684.602838701489, 4559.544892780475, 4121.971772379, 4125.482665710912, 4030.445745889172, 3814.904996302698, 3422.7298798083157, 3427.871538024062, 3540.768301270558, 3715.2693763807156, 3438.9898701717057, 3298.015920957153, 3085.922077280328, 2980.4989697689666, 2971.8155112145705, 3063.7129005124157, 2904.3866049426883, 2856.428537850715, 2740.9761087695993, 2902.6016174046426, 11103.848491042107, 3690.6503373783657, 17462.21577085493, 4403.507548496581, 4937.104708143738, 3491.667277517603, 4035.637534323927, 536.4940847669445, 445.19750808228935, 390.7659365466243, 475.8482004125918, 361.8072319206002, 290.6522139805379, 307.09687788488884, 298.966911540051, 320.47446200407643, 287.91868308847086, 282.2258958394218, 280.45027255574394, 278.89435258077384, 278.35924624228574, 334.6091102401152, 273.1272182005156, 241.97448693038166, 243.6229250517712, 363.6989098310549, 242.97276651588004, 223.9012696095907, 256.2057013567407, 264.85080813082476, 295.0852413659867, 261.0814039269466, 352.8412921744486, 274.178278245513, 253.4857265483737, 240.11655755740895, 228.56434040485377, 328.825844755454, 276.621917837599, 319.7412919617551, 311.8263440674468, 278.0396285623724, 541.9627919320504, 570.4634407677096, 1012.8153218382353, 384.12191508349633, 2152.924312843127, 379.66012471618535, 821.3452998379391, 1514.0736485487373, 1033.5324518736045, 3473.7092755836093, 363.3623260221881, 497.6495141204663, 602.2880453706118, 733.297257644353, 896.7228674124573, 1731.3981686427599, 17462.21577085493, 1475.3785568255114, 2067.507221001113, 974.9038892649949, 4937.104708143738, 3480.289630388219, 897.1461810121697, 3530.042424134693, 754.5301071163121, 961.7098443957686, 4035.637534323927, 1623.7050315847348, 1023.8665850287389, 2205.889220155918, 1073.6734676110707, 2376.4480889068777, 1421.2554879557229, 1141.511279150931, 2998.760784452884, 4403.507548496581, 2961.0349137493613, 2754.5130805834406, 795.1672625486389, 695.4570252390229, 414.0625303837245, 609.3783051815861, 330.93295232171107, 317.5536744676671, 271.0552478051219, 416.01616937651073, 333.8762689272976, 229.54429009507064, 245.81651329705144, 210.25706836152497, 237.56405560869064, 200.04730934565242, 179.13462283426315, 197.43021788831504, 175.2921155154966, 398.80420851373594, 183.48600029983533, 301.3126990077806, 279.70150954554293, 190.24805487739502, 152.2335235523302, 146.18873634173673, 209.33944643932028, 219.00863581454885, 144.59604208395677, 221.3284817164578, 153.22588960240333, 144.1091393078236, 195.40370633002988, 166.83058646077666, 162.99560799239137, 169.17188910070172, 217.95543865494622, 244.36749455945497, 227.0813329894969, 717.4982041636999, 466.63236861971905], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.2360999584198, -3.5078999996185303, -4.772500038146973, -4.936100006103516, -5.189700126647949, -5.251800060272217, -5.23829984664917, -5.234300136566162, -5.264200210571289, -5.2846999168396, -5.297100067138672, -5.32420015335083, -5.425099849700928, -5.424200057983398, -5.447500228881836, -5.502500057220459, -5.611000061035156, -5.609499931335449, -5.577099800109863, -5.5289998054504395, -5.606299877166748, -5.648200035095215, -5.714700222015381, -5.7494001388549805, -5.752399921417236, -5.72189998626709, -5.775300025939941, -5.791999816894531, -5.833199977874756, -5.776000022888184, -4.464700222015381, -5.542399883270264, -4.213900089263916, -5.5096001625061035, -5.5640997886657715, -5.703700065612793, -5.697800159454346, -6.041500091552734, -6.228600025177002, -6.359300136566162, -6.162600040435791, -6.436800003051758, -6.656099796295166, -6.601099967956543, -6.627999782562256, -6.558700084686279, -6.665900230407715, -6.6859002113342285, -6.692299842834473, -6.697999954223633, -6.699999809265137, -6.51609992980957, -6.719099998474121, -6.840199947357178, -6.833600044250488, -6.4328999519348145, -6.836299896240234, -6.918099880218506, -6.783400058746338, -6.750199794769287, -6.642099857330322, -6.764599800109863, -6.463500022888184, -6.71589994430542, -6.7947001457214355, -6.849100112915039, -6.898399829864502, -6.536099910736084, -6.708199977874756, -6.566800117492676, -6.592199802398682, -6.705900192260742, -6.08489990234375, -6.039000034332275, -5.5121002197265625, -6.433300018310547, -4.977099895477295, -6.457699775695801, -5.852700233459473, -5.383200168609619, -5.702700138092041, -4.8246002197265625, -6.504899978637695, -6.293799877166748, -6.170199871063232, -6.0644001960754395, -5.94379997253418, -5.544899940490723, -4.1280999183654785, -5.656599998474121, -5.546599864959717, -5.96019983291626, -5.115900039672852, -5.312099933624268, -6.063799858093262, -5.453100204467773, -6.152599811553955, -6.051599979400635, -5.524099826812744, -5.883200168609619, -6.066500186920166, -5.824900150299072, -6.085599899291992, -5.911200046539307, -6.02869987487793, -6.073800086975098, -5.92140007019043, -5.900899887084961, -6.046899795532227, -6.070199966430664, -4.459400177001953, -4.593800067901611, -5.113500118255615, -4.72730016708374, -5.338600158691406, -5.380199909210205, -5.538599967956543, -5.110300064086914, -5.33050012588501, -5.7052998542785645, -5.6371002197265625, -5.793700218200684, -5.671899795532227, -5.844099998474121, -5.954899787902832, -5.857699871063232, -5.976799964904785, -5.15500020980835, -5.93149995803833, -5.4359002113342285, -5.51039981842041, -5.896100044250488, -6.11899995803833, -6.159599781036377, -5.80079984664917, -5.75600004196167, -6.171299934387207, -5.745800018310547, -6.113699913024902, -6.175099849700928, -5.8719000816345215, -6.0289998054504395, -6.052700042724609, -6.026400089263916, -5.816100120544434, -5.720600128173828, -5.818299770355225, -5.551700115203857, -5.923600196838379], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.273, 0.273, 0.2729, 0.2728, 0.2728, 0.2728, 0.2728, 0.2728, 0.2728, 0.2728, 0.2728, 0.2727, 0.2727, 0.2727, 0.2727, 0.2727, 0.2727, 0.2727, 0.2727, 0.2727, 0.2726, 0.2726, 0.2726, 0.2726, 0.2726, 0.2726, 0.2726, 0.2726, 0.2726, 0.2726, 0.2421, 0.2659, 0.0402, 0.1222, -0.0468, 0.1601, 0.0212, 1.6954, 1.6948, 1.6945, 1.6942, 1.694, 1.6937, 1.6936, 1.6935, 1.6934, 1.6933, 1.6932, 1.6932, 1.6931, 1.693, 1.6928, 1.6928, 1.6928, 1.6927, 1.6926, 1.6926, 1.6926, 1.6926, 1.6925, 1.6925, 1.6925, 1.6924, 1.6922, 1.6919, 1.6917, 1.6916, 1.6903, 1.6911, 1.6876, 1.6872, 1.6882, 1.6418, 1.6364, 1.5893, 1.6377, 1.3702, 1.6249, 1.4582, 1.3161, 1.3784, 1.0443, 1.6216, 1.5182, 1.4509, 1.36, 1.2794, 1.0203, 0.126, 1.0686, 0.8412, 1.1793, 0.4015, 0.5549, 1.1589, 0.3997, 1.2432, 1.1016, 0.1948, 0.7462, 1.0241, 0.4981, 0.9575, 0.3374, 0.7339, 0.908, 0.0946, -0.2691, -0.0183, 0.0307, 2.884, 2.8835, 2.8824, 2.8822, 2.8814, 2.881, 2.8809, 2.8808, 2.8807, 2.8805, 2.8802, 2.8799, 2.8796, 2.8792, 2.8788, 2.8788, 2.8786, 2.8784, 2.8782, 2.8778, 2.8778, 2.8775, 2.8775, 2.8773, 2.8771, 2.8768, 2.8767, 2.8764, 2.8763, 2.8762, 2.8749, 2.8759, 2.8754, 2.8646, 2.8215, 2.8026, 2.7783, 1.8944, 1.9527]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.9994409069752818, 0.00032640134127213645, 0.00032640134127213645, 0.9218719657259171, 0.0005516887885852287, 0.07778811919051724, 0.999448213149528, 0.0003445185153910817, 0.0003445185153910817, 0.004466254263513857, 0.99597470076359, 0.004466254263513857, 0.0035855871255420215, 0.996793220900682, 0.0035855871255420215, 0.7986632386340752, 0.2014165328329776, 0.0003334710808493007, 0.48932156525641224, 0.5098320500276391, 0.0009766897510108028, 0.004104704020721755, 0.9933383730146647, 0.004104704020721755, 0.0051176101967638, 0.0051176101967638, 0.9876987679754134, 0.3412425523205176, 0.6579513263696254, 0.0011151717396095345, 0.0037757105861124787, 0.9967875947336944, 0.0037757105861124787, 0.00451817132727225, 0.00451817132727225, 0.9894795206726228, 0.011822295126168832, 0.005911147563084416, 0.981250495472013, 0.999786755065709, 0.000292164452094012, 0.000292164452094012, 0.21184756281472872, 0.7865145148179008, 0.00121751472882028, 0.003565694520055188, 0.9948287710953975, 0.003565694520055188, 0.0021015105219121017, 0.9961159873863361, 0.0021015105219121017, 0.6809778069348876, 0.3189389728682385, 0.00028733240798940404, 0.0029951215257462715, 0.0029951215257462715, 0.9943803465477621, 0.9999513282816999, 2.7187366184929305e-05, 2.7187366184929305e-05, 0.09688158736067291, 0.0044037085163942235, 0.8983565373444216, 0.004998817546064299, 0.004998817546064299, 0.9947646916667956, 0.006082246976320821, 0.991406257140294, 0.0030411234881604107, 0.003615042538267291, 0.994136698023505, 0.003615042538267291, 0.8599962548702647, 0.13988848508056062, 0.00022709169655935163, 0.005014992212478406, 0.002507496106239203, 0.9929684580707243, 0.0025590766913755414, 0.998039909636461, 0.0025590766913755414, 0.05350920843960064, 0.9465594458453493, 0.0018451451186069186, 0.005994104685564411, 0.005994104685564411, 0.9890272731181279, 0.0014379033695954228, 0.0014379033695954228, 0.9979049384992235, 0.999497074220065, 0.00033551429144681604, 0.00033551429144681604, 0.0035432609648583433, 0.9956563311251945, 0.0035432609648583433, 0.05964521959271137, 0.004588093814823952, 0.9359711382240862, 0.36446524453610474, 0.634832189573797, 0.0013253281619494718, 0.0027495270757465816, 0.9953288014202625, 0.0027495270757465816, 0.0038302230069201357, 0.9958579817992352, 0.0038302230069201357, 0.003021760126890798, 0.003021760126890798, 0.9971808418739633, 0.0035752399106633104, 0.0035752399106633104, 0.9939166951644003, 0.4481601207594921, 0.5511017726276817, 0.0010398146653352485, 0.06848230379589187, 0.9297789707673011, 0.0026339347613804566, 0.9997668812466913, 0.00020656340521625855, 0.00020656340521625855, 0.9995005882565309, 0.00028242457989729607, 0.00028242457989729607, 0.5457677128371385, 0.45378439044885677, 0.0008760316417931598, 0.4663209972905738, 0.5334224198658163, 0.0006777921472246712, 0.00691581861846101, 0.00691581861846101, 0.9889620624399245, 0.7922820437880005, 0.20770559977008152, 5.726650117730398e-05, 0.9996583353043562, 0.0002691594871578773, 0.0002691594871578773, 0.005256295527670167, 0.005256295527670167, 0.9934398547296616, 0.16929815699172138, 0.43931800232028967, 0.3943146947655283, 0.0036612974956814915, 0.9958729188253657, 0.0036612974956814915, 0.004756082674379143, 0.004756082674379143, 0.994021278945241, 0.004776930564253989, 0.004776930564253989, 0.9936015573648298, 0.006255056979751067, 0.9914265312905441, 0.0031275284898755334, 0.003388851287075132, 0.9963222784000888, 0.003388851287075132, 0.9997511188116, 0.00020127866293770887, 0.00020127866293770887, 0.9995225825169664, 0.00034430678006095984, 0.00034430678006095984, 0.9997005735163006, 0.00019173390362798247, 0.00019173390362798247, 0.7773245177048658, 0.22251750618392394, 0.00024779232314468147, 0.004375135676145762, 0.993155798485088, 0.004375135676145762, 0.6285730018316571, 0.001393731711378397, 0.3707326352266536, 0.006526312247850804, 0.006526312247850804, 0.9919994616733222, 0.0031490739374260292, 0.0031490739374260292, 0.9951073642266252, 0.0036472619435758136, 0.9957025105961971, 0.0036472619435758136, 0.0012575970454251843, 0.0012575970454251843, 0.9985320540675963, 0.002415094162404092, 0.002415094162404092, 0.99743388907289, 0.9997255848448556, 0.0003364946431655522, 0.0003364946431655522, 0.005582393756036812, 0.005582393756036812, 0.9936660885745525, 0.006939185153718496, 0.006939185153718496, 0.9923034769817449, 0.47960259993838994, 0.5201932161396582, 0.00028787671064729286, 0.9998881846973662, 0.00012634422348968487, 0.00012634422348968487, 0.7266201625406101, 0.2730845367209154, 0.00028328271444078365, 0.00641382627879384, 0.9909361600736483, 0.00320691313939692, 0.9997627735674742, 0.00026212972563384224, 0.00026212972563384224, 0.9996578496071753, 0.00021346526790672117, 0.00021346526790672117, 0.6985844918771933, 0.30146572816244877, 0.00045333192204879513, 0.8199835769331019, 0.179666912243991, 0.00033771975985712595, 0.005704763143848279, 0.005704763143848279, 0.9926287870296006, 0.002403752723118224, 0.002403752723118224, 0.9951536273709447, 0.00656885537866599, 0.00656885537866599, 0.9918971621785645, 0.9999398394050458, 3.567773359278716e-05, 3.567773359278716e-05, 0.05727348306908726, 0.9424091305004357, 0.002603340139503966, 0.21750390200653988, 0.7820178461456511, 0.001660335129820915, 0.9997121625218233, 0.00029078306065207193, 0.00029078306065207193, 0.004068074949836964, 0.004068074949836964, 0.9926102877602193, 0.003120373441760486, 0.995399127921595, 0.003120373441760486, 0.004115687590587, 0.995996396922054, 0.004115687590587, 0.00456602999366096, 0.00456602999366096, 0.9908285086244283, 0.003344851759175481, 0.9967658242342934, 0.003344851759175481, 0.9695737481185319, 0.03043989660635926, 9.005886569928775e-05, 0.07155392328265853, 0.9274489287021508, 0.002752073972409943, 0.999661173907308, 0.00021932013468786924, 0.00021932013468786924, 0.2863777244641618, 0.7132169042607459, 0.0013637034498293421, 0.27915519204032135, 0.7208799634718448, 0.00046448451254629174, 0.5225052280077556, 0.4768675164705362, 0.000931381868106516, 0.6177636656044722, 0.38135296897223675, 0.0007036032637864147, 0.9996438827881512, 0.0003648335338642888, 0.0003648335338642888, 0.0016410167403350766, 0.0016410167403350766, 0.9960971613833914, 0.002834135409258135, 0.9947815286496053, 0.002834135409258135, 0.0034405380447814515, 0.997756032986621, 0.0034405380447814515, 0.0039031137664169327, 0.9952940104363178, 0.0039031137664169327, 0.004132667095137635, 0.9959727699281701, 0.004132667095137635, 0.006135134635325143, 0.006135134635325143, 0.987756676287348, 0.9996476526449933, 0.00020400972502959046, 0.00020400972502959046, 0.0066376226643814805, 0.0033188113321907403, 0.9923245883250313, 0.3170255294120516, 0.6829258279417945, 0.0006604698529417742, 0.9994998867180518, 0.00035008752599581495, 0.00035008752599581495, 0.9996919599597148, 0.0003032126053866287, 0.0003032126053866287, 0.005450007076103328, 0.005450007076103328, 0.9919012878508058, 0.004209391010091081, 0.004209391010091081, 0.993416278381495, 0.9997751212442528, 0.00021083406183978337, 0.00021083406183978337, 0.9996412937971819, 0.0002481115149657935, 0.0002481115149657935, 0.004356457743234775, 0.004356457743234775, 0.9932723654575287, 0.9997642457462926, 0.0002426023406324418, 0.0002426023406324418, 0.0034732028824010796, 0.9968092272491099, 0.0034732028824010796, 0.0027639027409475697, 0.9950049867411251, 0.0027639027409475697, 0.0032563014215170128, 0.9964282349842059, 0.0032563014215170128, 0.006840472289618534, 0.006840472289618534, 0.9918684819946874, 0.007193219219652862, 0.9890676427022685, 0.003596609609826431, 0.7263366308769803, 0.27364216071243735, 0.00020254786137116016, 0.16276515439416722, 0.8359296818268341, 0.0020094463505452745, 0.05784770353660133, 0.9413399030046945, 0.0017529607132303435, 0.00186395344961614, 0.9972150955446348, 0.00186395344961614, 0.0022461940640852874, 0.9973101644538676, 0.0022461940640852874, 0.005065080769782128, 0.005065080769782128, 0.9927558308772971, 0.0777517485877291, 0.004092197294091005, 0.9207443911704761, 0.9997011987803851, 0.0003240522524409676, 0.0003240522524409676, 0.9997457495082899, 0.00029172621812322436, 0.00029172621812322436, 0.9997916469349174, 0.00014880066184475629, 0.00014880066184475629, 0.004164644080243871, 0.9953499351782853, 0.004164644080243871, 0.0035924799104018025, 0.9951169351812994, 0.0035924799104018025, 0.4041424024854868, 0.5959561823453499, 0.0010257421382880375, 0.6134119071047681, 0.3861538812798089, 0.0006158754087397271, 0.415762790829893, 0.5829596236033083, 0.0011146455518227694, 0.9996406079406815, 0.0002423958797140353, 0.0002423958797140353, 0.9930499139627011, 0.007044828857579871, 0.0002709549560607643, 0.8932695334640962, 0.10653936083637185, 0.0002863961312805695, 0.0029885617856680617, 0.9951910746274645, 0.0029885617856680617, 0.003689284779016567, 0.003689284779016567, 0.9961068903344732, 0.003944995300590094, 0.9941388157487039, 0.003944995300590094, 0.5750886806694059, 0.42466598959439733, 0.0004836742478296097, 0.10169672375518322, 0.8974982708103063, 0.0009873468325745944, 0.7431258474542684, 0.2566855984977937, 0.0004207960631111372, 0.4915102807733114, 0.507682181903338, 0.0005775678975009534, 0.999753799102451, 0.0002004719869866555, 0.0002004719869866555, 0.8110326343147409, 0.18878109661757622, 0.0003630405704184158, 0.27285064875204046, 0.7266341745134126, 0.0009675554920285122], \"Term\": [\"acting\", \"acting\", \"acting\", \"action\", \"action\", \"action\", \"actor\", \"actor\", \"actor\", \"adaptation\", \"adaptation\", \"adaptation\", \"african\", \"african\", \"african\", \"also\", \"also\", \"also\", \"american\", \"american\", \"american\", \"antwone\", \"antwone\", \"antwone\", \"army\", \"army\", \"army\", \"art\", \"art\", \"art\", \"arthur\", \"arthur\", \"arthur\", \"attack\", \"attack\", \"attack\", \"attacked\", \"attacked\", \"attacked\", \"bad\", \"bad\", \"bad\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautifully\", \"beautifully\", \"beautifully\", \"beauty\", \"beauty\", \"beauty\", \"best\", \"best\", \"best\", \"blood\", \"blood\", \"blood\", \"br\", \"br\", \"br\", \"btk\", \"btk\", \"btk\", \"bullet\", \"bullet\", \"bullet\", \"cagney\", \"cagney\", \"cagney\", \"capture\", \"capture\", \"capture\", \"character\", \"character\", \"character\", \"chase\", \"chase\", \"chase\", \"cinderella\", \"cinderella\", \"cinderella\", \"city\", \"city\", \"city\", \"cliffhanger\", \"cliffhanger\", \"cliffhanger\", \"cop\", \"cop\", \"cop\", \"could\", \"could\", \"could\", \"culture\", \"culture\", \"culture\", \"dahmer\", \"dahmer\", \"dahmer\", \"dance\", \"dance\", \"dance\", \"de\", \"de\", \"de\", \"delight\", \"delight\", \"delight\", \"detective\", \"detective\", \"detective\", \"disney\", \"disney\", \"disney\", \"drama\", \"drama\", \"drama\", \"emotional\", \"emotional\", \"emotional\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"ever\", \"excellent\", \"excellent\", \"excellent\", \"family\", \"family\", \"family\", \"fighting\", \"fighting\", \"fighting\", \"film\", \"film\", \"film\", \"first\", \"first\", \"first\", \"fly\", \"fly\", \"fly\", \"force\", \"force\", \"force\", \"french\", \"french\", \"french\", \"fu\", \"fu\", \"fu\", \"gang\", \"gang\", \"gang\", \"german\", \"german\", \"german\", \"germany\", \"germany\", \"germany\", \"get\", \"get\", \"get\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"great\", \"great\", \"great\", \"handsome\", \"handsome\", \"handsome\", \"hero\", \"hero\", \"hero\", \"hunt\", \"hunt\", \"hunt\", \"island\", \"island\", \"island\", \"italian\", \"italian\", \"italian\", \"killer\", \"killer\", \"killer\", \"killing\", \"killing\", \"killing\", \"know\", \"know\", \"know\", \"kung\", \"kung\", \"kung\", \"legend\", \"legend\", \"legend\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"love\", \"love\", \"love\", \"lovely\", \"lovely\", \"lovely\", \"made\", \"made\", \"made\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"many\", \"many\", \"many\", \"military\", \"military\", \"military\", \"monster\", \"monster\", \"monster\", \"mountain\", \"mountain\", \"mountain\", \"movie\", \"movie\", \"movie\", \"moving\", \"moving\", \"moving\", \"mr\", \"mr\", \"mr\", \"much\", \"much\", \"much\", \"murder\", \"murder\", \"murder\", \"musical\", \"musical\", \"musical\", \"nature\", \"nature\", \"nature\", \"officer\", \"officer\", \"officer\", \"oliver\", \"oliver\", \"oliver\", \"one\", \"one\", \"one\", \"oscar\", \"oscar\", \"oscar\", \"people\", \"people\", \"people\", \"perfect\", \"perfect\", \"perfect\", \"performance\", \"performance\", \"performance\", \"picture\", \"picture\", \"picture\", \"play\", \"play\", \"play\", \"plot\", \"plot\", \"plot\", \"police\", \"police\", \"police\", \"political\", \"political\", \"political\", \"portrayal\", \"portrayal\", \"portrayal\", \"portrayed\", \"portrayed\", \"portrayed\", \"powerful\", \"powerful\", \"powerful\", \"ray\", \"ray\", \"ray\", \"really\", \"really\", \"really\", \"rex\", \"rex\", \"rex\", \"role\", \"role\", \"role\", \"say\", \"say\", \"say\", \"scene\", \"scene\", \"scene\", \"scientist\", \"scientist\", \"scientist\", \"sea\", \"sea\", \"sea\", \"see\", \"see\", \"see\", \"seen\", \"seen\", \"seen\", \"serial\", \"serial\", \"serial\", \"show\", \"show\", \"show\", \"singing\", \"singing\", \"singing\", \"social\", \"social\", \"social\", \"society\", \"society\", \"society\", \"soldier\", \"soldier\", \"soldier\", \"stage\", \"stage\", \"stage\", \"story\", \"story\", \"story\", \"strong\", \"strong\", \"strong\", \"superb\", \"superb\", \"superb\", \"supporting\", \"supporting\", \"supporting\", \"tale\", \"tale\", \"tale\", \"terrorist\", \"terrorist\", \"terrorist\", \"theodore\", \"theodore\", \"theodore\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"tony\", \"tony\", \"tony\", \"touching\", \"touching\", \"touching\", \"true\", \"true\", \"true\", \"u\", \"u\", \"u\", \"war\", \"war\", \"war\", \"watch\", \"watch\", \"watch\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"welles\", \"welles\", \"welles\", \"werewolf\", \"werewolf\", \"werewolf\", \"western\", \"western\", \"western\", \"woman\", \"woman\", \"woman\", \"wonderful\", \"wonderful\", \"wonderful\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"would\", \"would\", \"would\", \"year\", \"year\", \"year\", \"young\", \"young\", \"young\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el225031403256893884962092373774\", ldavis_el225031403256893884962092373774_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el225031403256893884962092373774\", ldavis_el225031403256893884962092373774_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el225031403256893884962092373774\", ldavis_el225031403256893884962092373774_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.271085  0.097782       1        1  76.106510\n",
       "2     -0.019351 -0.219992       2        1  18.311544\n",
       "1     -0.251734  0.122210       3        1   5.581946, topic_info=              Term          Freq         Total Category  logprob  loglift\n",
       "25277           br  36781.000000  36781.000000  Default  30.0000  30.0000\n",
       "306          movie  28028.000000  28028.000000  Default  29.0000  29.0000\n",
       "51            film  17462.000000  17462.000000  Default  28.0000  28.0000\n",
       "290           life   3473.000000   3473.000000  Default  27.0000  27.0000\n",
       "1433   performance   2152.000000   2152.000000  Default  26.0000  26.0000\n",
       "...            ...           ...           ...      ...      ...      ...\n",
       "34303       dahmer    204.422843    217.955439   Topic3  -5.8161   2.8215\n",
       "7998      theodore    224.904434    244.367495   Topic3  -5.7206   2.8026\n",
       "34446          btk    203.973005    227.081333   Topic3  -5.8183   2.7783\n",
       "1000          hero    266.273537    717.498204   Topic3  -5.5517   1.8944\n",
       "2654         force    183.581309    466.632369   Topic3  -5.9236   1.9527\n",
       "\n",
       "[179 rows x 6 columns], token_table=      Topic      Freq    Term\n",
       "term                         \n",
       "176       1  0.999441  acting\n",
       "176       2  0.000326  acting\n",
       "176       3  0.000326  acting\n",
       "2         1  0.921872  action\n",
       "2         2  0.000552  action\n",
       "...     ...       ...     ...\n",
       "408       2  0.188781    year\n",
       "408       3  0.000363    year\n",
       "1130      1  0.272851   young\n",
       "1130      2  0.726634   young\n",
       "1130      3  0.000968   young\n",
       "\n",
       "[438 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I first tried using 8 topics with my LDA visualizatoin, but that seemed like too many because there were overlapping ones. I decided that there were three topics.\n",
    "# It looked like there was one for action films, one for drama/romance films, and one that talked about audience reactions to the films. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
